# 基于 BERT 微调的中文文本情感分类

## 项目描述

该项目基于预训练模型BERT开发，实现了中文文本的二分类情感分析任务。项目采用迁移学习策略，在BERT的预训练权重基础上，通过添加全连接层并冻结上游模型参数的方式进行微调，以降低计算成本。数据处理模块使用BertTokenizer对文本进行编码和填充；模型架构在BERT输出的CLS标记的基础上进行分类，通过Softmax激活函数输出概率分布；训练采用交叉熵损失函数和Adam优化器，在ChnSentiCorp数据集上训练，实现了92.35%的分类精度。
